# Making Observability Actionable at Scale
## DBS DevConnect 2019

[Abstract](#Abstract) [Video](https://www.youtube.com/watch?v=In_r6IIdUT4) [Slides](https://www.slideshare.net/squadcastHQ/making-observability-actionable-at-scale-dbs-devconnect-2019) [Transcript](#Transcript)

## Abstract 
Many organisations already possess a vast amount of existing data about production systems. As customer expectations evolve, organisations are often challenged to find more proactive ways of dealing with traditionally reactive incident response activity. In this talk, we discuss approaches to unlock value from this data by making it truly actionable. Understanding production failure modes better, enriching technical and business context effectively, decomposing response activity into shared primitives, actions and workflows, and overall, sharing and augmenting this active knowledge repository on a continuous basis are key takeaways. Through case studies, we'll discuss how we can accomplish this by engineering your observability processes and tooling to work for human-in-the-loop interpretation and response rather than a purely human-reliant strategy.

## Transcript

**Sisir Koppaka**:      00:02          Thanks Harpreet. Thanks to the SRE team for hosting me at DBS. This is actually a very interesting topic and learning that we have had at Squadcast, and I wanted to kind of share the data and how did we arrive at some of these learnings over the last 18 months with all of you. So I do have to use the product that I built which captured the data to illustrate it. So don't consider it a plug. We'll stick to kind of discussing the concept and you might be able to pull it off in your own domain with your own tools, something similar.

**Sisir Koppaka**:      00:37          So what I want to talk about today is how to make observability actionable at scale. Okay. And I'll kind of go into each of these concepts in the talk and explain what it means because I don't think that's the case today, and there is data to prove that, that I'll kind of bring up and show. So let's go ahead. Yeah. Just to quickly summarize who I am. So I went to undergrad at IIT Kharagpur. I studied reliability engineering there. I was actually not in the CS department. I was in the mechanical engineering department. So a lot of the learnings that, if you kind of go into the history of how SRE evolved, a lot of these things have been in other industries for some time. So ... Even if you look at something like Kanban. Kanban ... if you look at Trello. Trello allows you to practice certain amount of Kanban, but Kanban goes there in [inaudible 00:01:38] production system.

**Sisir Koppaka**:      01:40          So in my undergrad, I was exposed to some of these concepts and quality engineering and these MTTR ... A lot of these things are more or less well known in that field. So that's kind of where I got plugged into this entire journey. And after that I tried to kind of apply some of those learnings in healthcare at MIT. I went to grad school there and we tried to make disease diagnosis automated and more reliable.

**Sisir Koppaka**:      02:05          So in case you don't have a lot of [inaudible 00:02:07] in the US, around 75% of them at any point in time, are available for doing any ultrasound scans because they have a repetitive stress injury. Because they kind of have to manually press the ultrasound. It's not automated. So in a majority of the sonographers are not available, get billed. So it's a huge loss. And we tried automated by building a reliable software and hardware to automate that. So you can ... The answer was yes. So, that was an interesting learning apply it to healthcare.

**Sisir Koppaka**:      02:37          And after that, at the time I was in the CS department, I wrote the software for that project. And then I worked at a startup in New York on calendar scheduling. So if you heard of x.ai and Calendly, must be using any of these tools. So I was at x.ai and we built an AI agent to automate scheduling of calendar events. So that's kind of where I was also on call. And because it's a startup, there is only like 40, 50 engineers and I had to practice some of ... apply SRE there. And when I worked in [inaudible 00:03:12] as well, in a tech startup in the logistics space, it was the same learning. So there's been a lot of learnings I've had in how to operate reliable systems at scale. And we kind of started the journey to productize a lot of this so that everyone has access to SRE best practices without having to kind of rebuild a lot of big core technologies that are required.

**Sisir Koppaka**:      03:34          So what I'm not, I'm not an expert in banking, so you'll have to figure out how to apply this best to your ... this learnings to your area. But what I shared is what we have learned across industries.

**Sisir Koppaka**:      03:45          So, this is a free tool. Okay. We built a free tool that kind of allows you to ... It's something similar to Postman. In fact, one of our investors is also the same investor as Postman, if you have used Postman. So it's a free tool that allows you to practice SRE. And it allows you to kind of engage with your system and the software development life cycle through different interfaces, whether it's Slack, whether it's command line, whether it's our web app, mobile, when you're on the go, and kind of manage the reliability of a system more effectively.

**Sisir Koppaka**:      04:16          So you want to democratize SRE and a lot of the concepts that you see, SLOs, service definitions, error budgets are kind of what we want to make it easily configurable in a declarative way and available. But what we are going to stick to this in this talk is only the actions part. Okay. How does actions ... something that we call actions make you improve your observability in your systems.

**Sisir Koppaka**:      04:42          So just before we go there, just a quick look at what an SLO looks like. So an SLO for latency is something like it should be less than ... this metric should ... this SLA should be less than a hundred milliseconds for 99 percentile of requests over the period of sliding time window of one hour. So this is what an SLO looks like and whenever this is breached, you have an incident, okay, in production.

**Sisir Koppaka**:      05:02          Now the thing is typically this is siloed to just the production response teams but not the software engineers. But it's actually very important to think of this more as a holistic one integrated piece. And because without that you will be stuck in a vicious cycle where you will be toiling, engineers will be just operating, maintaining code in production not really building new features. And there's a lot of reparative activity that kind of gets you to lose a lot of valuable time that you could be spent better serving your customers.

**Sisir Koppaka**:      05:34          So, we have been collecting a lot of data across customers. We have a lot of users around the world. So these are some of the customers that I just put together yesterday just to show you what kind of companies are trying to practice SRE. So there are logistics companies, there's like bike taxis, there's like telecom. There is Verizon Connect, which is the fleet management division of Verizon. A lot of these customers have been using us, including media companies. There's a bank, Discovery Bank From South Africa. So we have collected a lot of this data and I'll show you what the data says, okay, as we go into the slide presentation.

**Sisir Koppaka**:      06:14          So what does observability really mean? Okay. So to be very, very ... let's keep it very, very simple. Observability means that given the input and output of the system, okay, if something changes in the output in a negative way, it should be able to deduce what caused that change within the system. Okay. So you might have like a hundred microservices and something went wrong. Something went red. Maybe one of your customers opened a ticket. And you need to be able to go back and say that, "Okay, that was caused by so-and-so." Issue within your software. So it sounds very simple, but actually it's not. It's more easy to understand when you look at what excellent observability and not so good observability looks like.

**Sisir Koppaka**:      06:59          So when you have excellent observability you have like very proactive customer success. So you reach out to your customers to inform them that there is an ongoing issue. Okay. So if you actually calculate the number of times that probably happens, you'll get an idea of where you are on the scale of observability kind of effectiveness. You have very high time between failures. So the same failure does not occur again and again every week when you have excellent observability. When you have very low time to act and low time to resolve, so you are able to fix the issues that occurred ... very fast. When you are extremely transparent within your organization, everyone has access to the same kind of knowledge about what is actually the reality in production. So that the subject matter expert, if it is a software engineer, they can be looped in and fix the issue. So you need some kind of transparency. And it allows you to have very predictable change velocity.

**Sisir Koppaka**:      07:58          So you should not be like you plan a certain number of change requests for this quarter and then you don't end up doing it because they would a lot of like ongoing maintenance issues that you had to fix. So that's a sign that you need to improve and invest in your observability stack. And you experience a lot of low toil. So engineers who are supposed to work on features will work on features, not like fixing bugs when there is excellent observability. And the best thing is you get sticky customers, customers who use you and recommend you again and again and again, which I'm sure DBS has. So, but generally it's like when your customers start complaining and you don't know that there is an issue, that's a sign that there is a scope in that one area to improve your observability.

**Sisir Koppaka**:      08:40          So what are the main pillars of observability? So this is a very kind of like a different companies will say different things. [inaudible 00:08:47] will say something, and a lot of companies will say different things, but let's just go with a very high level thing. A definition of what the main key components are. So if you say, let's say ... let's define what it means to have good observability. If you're able to kind of detect where in your system there is an issue, with the trace. If we're able to detect when it does happen with a metric and if you're able to say how it happened, what is the root cause with a log message, then you have some kind of key components of the observability in place. And of course this needs to work with a lot of your scenarios that you have in place, a lot of your products.

**Sisir Koppaka**:      09:24          But what we have understood is this is not just ... this is not enough on its own. So, and I'll explain why with examples and with kind of data. So we have analyzed a lot of the data of time to act and time to resolve for customers across the world. And what we've seen is that if you normalize this time to act and time to resolve, by shift. Okay. So if you divide the day into three shifts and you look at how these metrics operate in each of those shifts, and everyone has a serious SRE, they're all on-calls, they're all on call, they're like doing their best. But in spite of doing their best, you definitely see a difference between the response patterns within working hours and in other shifts. And the main issue is that it not only is it different, okay, not only are these numbers like significantly higher, in addition sometimes they're not consistent. So it might not be consistent month on month. So you can't plan for it.

**Sisir Koppaka**:      10:22          It's not transferable. So SREs who are like working on one product operate with different kind of TTR, TTR metrics on a different product. It's not very transferrable and it's not scalable. So if you have a lot of subject matter experts that need to get involved and take some action during the incident, it prolongs the incident response times. So it's very similar to how CSED was in 2005. Before you had declarative configuration for continuous deployment. So are we at peak observability today as a community? If you remember peak oil, when oil utilization was a maximum, and then peak solar, so when solar utilization will be maximum. So if you define something called peak observability where you have the best observability possible. How do we say whether we have done the best that is possible to do for this problem? And what we think is that no, we are not there yet because if we contact effectively, okay, if we cannot respond effectively when we see the data that we have on the telemetry, then we cannot really claim that we have peak observability.

**Sisir Koppaka**:      11:28          That's what we think should be the case because ultimately you have to hold yourself to the truth, which is that the data should be showing that you are able to operate effectively. Then of course you have solved the observability problem. But until that happens, there is some more work to do. So we also did a few surveys and we kind of got a sense that ... We did a survey of just SREs, people who identified themselves as SREs from user ex conferences ... just to give you an idea, more than half of them were managing between 50 to 500 services in production and they were around 32% managing 10 to 50 services in production. And the question that we asked them was, how many of your incidents do you think that recovery or the resolution could be automated? Either by triggering some actions in some other tools or by running some predefined scripts that you might already have in your enterprise, like maintained over several years. But it's just not possible to get all of it to work together and transparently be automated in a secure way.

**Sisir Koppaka**:      12:35          So it seems like there is a quite a bit of almost everyone agrees that there is ... A significant majority of SREs agree that there's at least 25% of run books that could potentially be run and prevent incidents from happening. So, and there's a significant portion, at least 25% of people, something around that nature where they feel more than half of that incidents will be moved if-

**Sisir Koppaka**:      13:03          More than half of the incidents will be removed if they were able to automate it effectively. There's quite a bit of demand for it, and need for it in the market. A lot of our understanding when we simplify it, it's something like this, and this is just a funny code. I don't have any affiliation with anything. It's just like the point is that if you have all the data, but you don't act on it in a scalable way, and you still are tactical about it, it's not going to allow you to achieve peak observability.

**Sisir Koppaka**:      13:39          A lot of the telemetry data is essential, but not sufficient. Okay. So you do need to have some kind of notion of what an action is, who are taking these actions in your organization to respond to incidents? How do you make this more traceable through the Software Development Life Cycle, which we'll do a demo off.

**Sisir Koppaka**:      13:59          There's a lot of other aspects, but primarily the idea is that observability is when it's bi-directionally linked. All these three pillars are bi-directionally linked to how you execute and respond to incidents. Only then you actually have a very good system where you can continuously improve it.

**Sisir Koppaka**:      14:18          What are these actions? We'll do a quick demo here, but before we just get into the demo and the GUI, I just explained what they are. The way we have gone ahead and defined it is that there are these ... you have these namespaces that might be tools that you use, or it might be a custom namespace where you might execute certain runbooks that you already maintained, that we provide the infrastructure for how you ... We build the infrastructure for how you execute it in production.

**Sisir Koppaka**:      14:45          You can execute it from the command line. Like if you see this, they're executing a rebuild in response to some incident. That gets logged to the incident timeline so that when you do the postmortem analysis, you don't have to recreate the timeline again. All you have runbooks, which we see in lot of large companies. In a smaller company, usually what happens is you have a lot of external tools that they might be using, which they can just click and respond to an incident, and fix it, go back, do a rollback and all.

**Sisir Koppaka**:      15:21          But the problem is that in a lot of large companies, you have a lot of custom scripts. These custom scripts are often comprised like significant percentage of your response remediation strategies. For this, there's a long tail of response activity where you have different ways of responding to a large number of incidents, and different combinations of them, different workflows, and different sets of people need to be involved.

**Sisir Koppaka**:      15:47          For that, what we have done is we've decided to build a markdown-enabled active runbook concept where you can plug in any language, any shell script. You can use shell scripts, you can use a programming language that you're familiar with, you can use Ansible playbooks, and it's markdown-enabled, so you can make it kind of like confluence, but more active. You can hit run and execute it, and I'll show you that with a hello world version, just so that you get an idea before we go to the next part of the presentation.

**Sisir Koppaka**:      16:18          Let's say we just load the web app, and I'm showing you this because I just want you to get a feel of how you edit it, but you can execute it from the CLA as well. This is the hello world runbook, and what it looks like is there is markdown on the left side, and there is a shell ... You can choose which language you want to use, which interface you want to use. In this case, we're using shell scripts.

**Sisir Koppaka**:      16:42          It's using a variable called dollar incident, which basically contains the real time information of the incident at that point when this runbook is executed. It contains all the incident data, which would keep the rating, right? It would keep changing as the incident progresses. But what we do is we provide that as a variable within the code so that you can act upon it, and do things programmatically instead of ... It can be code, you can respond with code.

**Sisir Koppaka**:      17:15          Let's just go to an incident and execute this and see how it looks like. Let's say we go to this incident, and we decided to run. In this case, we are using an agent that we provide, and there are three actions enabled for this particular organization. Let's just run the hello world and see. You can see there are some snippets here. This runbook is an example runbook. It has like five different kinds of languages that could be used within the same runbook, just to illustrate stuff.

**Sisir Koppaka**:      17:45          You can pick like, okay, I want to execute the bash version, and I want to execute the node version, and in this order. This is one and this is two in response to this incident. When you hit run, it triggers the runbook. You can follow it here by clicking there, or you can see it in the incident timeline. When you go and click on the runbook execution, you can actually see that the left side is the executed markdown, and the right side is basically the code.

**Sisir Koppaka**:      18:16          This is the Jason of the dollar incident variable, the real time data of what the incident contained. This could be coming from your production code, so your production code could be sending critical feature flags, or something else that you want to highlight to the person who is on call. You can see the past executions of this particular runbook as well. You can go back and see what happened in response to which incident.

**Sisir Koppaka**:      18:44          Now that we have a basic idea of what an action is, and what a custom action, which is a runbook is. Let's go back to the slide deck and continue. Some highlights of what we learned while building these actions and using them. It's the same idea as in programming. You have DRY, right, Don't Repeat Yourself. It's called be DRY, not WET. WET is Waste Everyone's Time, so innovative, repeat the code.

**Sisir Koppaka**:      19:11          It's the same concept where you build the shared primitives and share them in your organization in a transparent way. Then, it also enables you to create audit trails with a very immutable log, so you know exactly what happened in which order, and who did it, and so that you know how to fix the root cause later, you need to have this in place without relying on he'd say, or a meeting to happen to find out what happened.

**Sisir Koppaka**:      19:39          Continuous security. Today, there's a lot of dependencies that we use on the open source world. You can't expect that there are no vulnerabilities in your code. You have to be able to respond continuously to security issues, and immediately fix those issues, roll back, or whatever you need to do.

**Sisir Koppaka**:      19:59          And composition. If you look at languages like Scala, which are built on the JVM, but allow better composition capabilities to allow you to write more functional code. The same concept applies here. If you have a way of responding to an incident that contains like five different sub-snippets that you might want to use, then you should. Sorry, I'm just not feeling that well. Sorry for the cough.

**Sisir Koppaka**:      20:27          The thing is in your development lifecycle, right, you want to have a continuous feedback loop. I'll give you a demo of what that means. If the output of the programmers and the software engineers is to generate a build artifact, then they should have a very clear annotation of how that artifact has performed in production so that you can actually analyze it, you can trace it back, you have good traceability between the people who are writing the code, the people who are testing the code, the people who are shipping the code, and maybe the person who is directly responsible for it in production, and then the SREs who are making sure that this whole cycle continues to improve and not degrade over time.

**Sisir Koppaka**:      21:06          The thing is, as your business requirements change, so one of the projects that I worked on earlier was we built an AI agent that automated scheduling of meetings, right? Google built a version of that called Google Duplex where you could schedule, like you could talk to an AI over the phone and get flowers delivered to you without ... It's a very efficient way to do things over voice. But the thing is, eventually, in banking also there'll be a point where you'll have to ... the business requirement might be something of that nature, which will change your workload.

**Sisir Koppaka**:      21:37          Okay, so the workload that you're running on your cloud will change, and when you have heterogeneous workloads, you will have to start figuring out how to optimize the SRE process for heterogeneous workloads, not necessarily the particular workload that you are running today. The other thing is in a lot of regulated industries that we see in telecom, defense, banking and financial services, there is a lot of regulatory requirements. This entire infrastructure cannot be working only for the cloud or only for on-prem. It has to work for the hybrid environments. Wherever you might be running your code, whatever you might be doing, the process needs to be the same.

**Sisir Koppaka**:      22:19          Let's look at an example, and we'll quickly dive into two more small demos where you'll get an idea, and we'll go from there. This is a case where it's a well-known database company, which is also red. It uses a red color, and it's a Fortune 100 company. This company has 100 terabytes of artifacts that are being generated by its pro software engineers, and it continues to increase every year, but there's no clear traceability once it's been versioned in say, I think you're using Sonatype, so if something like Sonatype is there, and once it hits Sonatype, it's not clear exactly how it's being used in production for this particular company.

**Sisir Koppaka**:      22:57          And then of course they have different engineering teams for each product line. They have an NOC team, they have a separate SOC team. The question that we have here, and the answer will be the two short demos that we do, is can we unlock more value out of this data that they have already in their development life cycle by taking more actions in a semi-automated way or an automated way? And how will that improve observability? That's the main focus here.

**Sisir Koppaka**:      23:24          There are some use cases that I've listed, and we'll execute these runbooks now, after this. How can you automatically flag a build artifact when there is a telemetry spike? How can you inform ... Let's say you have all this metadata on your build artifact, how can you actually enrich it the right way so that there is good traceability to what happens in production? That's one of the demos that we'll be doing.

**Sisir Koppaka**:      23:48          The second thing is when you have vulnerabilities, okay, so there are tools to automatically detect vulnerabilities in your code that exist because of dependencies, not necessarily the effort that you did, but because of the code that you depend upon, and how do you automatically flag them and initiate automatic rollbacks without having to do a manual review every time?

**Sisir Koppaka**:      24:11          The third thing is, of course, when you have external events, say there's a holiday season, and people are using you a lot more than they used to, and you need to do capacity planning differently for that, can you automate some of this? These are all different kinds of failure modes in production, and we'll categorize this a little bit before we go ahead.

**Sisir Koppaka**:      24:32          When you have a development lifecycle that looks very simple for a very simple workload, there is no like ML here. There is no regression testing required. It's a very simple workload where multiple developers are working on a VCS, and then pushing artifacts that are different checks and balances along the way before it hits production.

**Sisir Koppaka**:      24:51          Of course, there is a backward pressure on this entire cycle where there is the SRE process where every time there's an incident, we have to initiate this entire set of events where we analyze why the SLO breaches happened. We look at whether it was routed to the right person, we'll try to triage the incident, and we try to remediate the incident before we find the root cause, and then eventually, we might want to find out in a post-mortem what could we have done better. Improving the observability of this through the process can really remove the drag force that you have. You can ship a lot more changes.

**Sisir Koppaka**:      25:26          The example that we are looking at in the demo here is how can we dramatically increase the traceability in our development lifecycle by doing two things in the runbook? One is, whatever is the realtime metadata that we're generating for an incident, we try to back-propagate it, we try to bring it back to the build artifact so that you know exactly what set of code changes resulted in those kind of production issues. This will be in an automated way, so there won't be any kind of manual work.

**Sisir Koppaka**:      25:55          The second thing is using this metadata to programmatically drive the incident response. Let's say there is an issue with a particular feature, and the build artifact ...

**Sisir Koppaka**:      26:03          So let's say there is an issue with a particular feature and the build artifact has been flagged that this particular feature is no longer working. And this is for service A. Now there's a service B that kind of depends upon this feature working in service A, in order to work correctly. Then you can initiate an automated rollback of all the dependent services also, like service B. So this is what is meant by programmatic response to incidents that you have in production. So this is all possible with a quick demo that we do. So if you look at the other run book that we have, so in this case I'm using JFrog Artifactory. You can also do the same thing with Sonatype because they also have the same metadata that could be enriched in the same way. You just have to write different code. So here we just look at this run book where we are taking this incident variable. Sorry, I don't know if it's visible. So let me just zoom this in. And you can take a lot of the data about the incident and you might have a release tag. You might take the release tag and you want to put that in a flag that this should not be used anymore. This artifact should be pulled from production everywhere, wherever it is deployed, whichever geography it's deployed. And at the JFrog command line tool that we are kind of using to write it to JFrog.

**Sisir Koppaka**:      27:23          And this is something that kind of will be executed in response to this incident. And you can see it's the same thing as we saw earlier except that it's doing two things which you can see here. So we are setting the incident properties and we are also marking this release as unsafe because this should not be used anymore. So the moment this is done, there will be a set of actions initiated to roll back everything back to what it was earlier. And you can track how this is being executed. So it has executed. So, yeah. So this is basically a quick example of how you can automate this with every incident that you're having in production. So I'll quickly run through the remaining stuff.

**Sisir Koppaka**:      28:11          So basically you have actions and run books and this needs to be a human in the loop system. So you don't want to do things automatically unless a human has approved. And you can compose these primitives and it needs to work on all interfaces including your mobile phone. So if you use the mobile app, you should be able to respond from the mobile app and execute a series of actions that are approved series of actions.

**Sisir Koppaka**:      28:34          So the failure modes are of four kinds, in general. So there are the known knowns, like you know that there will be a telemetry spike and you need to automate this. There will be some unknown knowns. For example, you don't know whether there'll be a vulnerability or a security issue. You want to prepare for it and execute it on demand. There are known unknowns and there are unknown unknowns. So known unknowns are, you don't know when traffic from the customer will increase. And unknown unknowns are the ones that you should focus your heart on, time on and convert these unknown unknowns to with one of these other three types.

**Sisir Koppaka**:      29:06          So I've given you an example of how you could do the known knowns. So I'll just run through another example. So the first example we already saw, traceability can be improved. Second example is enriching the production context by kind of executing actions to automatically annotate your visualization tools. So that's a small video that we'll just see. And the third thing is something that you can try later, which is how do you automatically take vulnerability data from some tool like Snyk and initiate rollbacks? So this is a short video that kind of shows you how to annotate your visualizations with incident data automatically.

**Sisir Koppaka**:      29:51          So you see... So you're responding to this incident. And you've decided to annotate Grafana with that particular services, annotations. And when you execute this, you can see that it's being executed and this is your graph on a dashboard that somebody else might be looking at. And suddenly you've got the incident data here, automatically. So this incident data contains the tags that are associated with that incident. You can also pull any other data with that incident, say, the severity level, or the service context, or which deployment was responsible on which subject matter expert to call.

**Sisir Koppaka**:      30:41          So it's kind of automates everything. So that variable you're working, all the people have access to the same information in the right context. And there are more things if you search on the internet, there are more such needs. So there is somebody asking for an annotating Grafana with puppet code deploys, and please tell me how you did it. So that's another thing that you could try. And you can automate all this and you can have a easier time dealing with incidents. So actions, basically what we have learned is they help you make your system be more observable. And that's kind of where I want to end. I don't know if I ran a little bit over time or I'm on time. So thank you so much for your patience. Hopefully it will useful.

**Harpreet Singh**:     31:20          Okay. I think we have a few interesting questions. So we'll take the first one. So can we log a successful stack trace from the start of a thread until completion for high observability?

**Sisir Koppaka**:      31:32          Yes. There are some tools that we already have direct integrations with like New Relic, we already logged them into the same service. So you can attach multiple sources of causes of this incident. Triggers. So some of them could be coming from your application code, some of them could be coming from the New Relic that you could... ABM that you could be using. So we can see all of that information in one incident data. So the answer is yes you can, but we don't do a lot of work to make that happen. We just integrate with a different tool basically.

**Harpreet Singh**:     32:04          Okay. I think there are a few product comparison questions as well. So can the same functionality be achieved using Jira?

**Sisir Koppaka**:      32:13          So this is a good question. So what problem we are solving is to basically help you optimize for reliability. So Jira is a different tool. We integrate with Jira. A lot of customers actually, customers that I mentioned, they use the Jira integration. We are on the Jira marketplace. So the idea is that change requests that are traveling through the organization needs to be tied together in some place. But what we are solving is the reliability aspect and which is a different concept.

**Sisir Koppaka**:      32:43          So from Jira you could potentially maybe build a separate product, internal product may be that integrates with Grafana and does the same thing but it won't have the same... You won't have run books and actions and lot of the other things like SLOs, and making sure that based on the SLO you can initiate certain other activities such as... So we can, for example, put a postmortem kind of ticket it Jira based on an incident but we don't replace Jira.

**Harpreet Singh**:     33:12          Yeah, a few product specific questions as well. So is Squadcast agent-based?

**Sisir Koppaka**:      33:21          That's the other thing. So see, the thing is, I know that a lot of the questions are about the product, but the idea that I wanted to talk about this about observability and how actions in any context you might be using your own internal framework as well will help you. So to answer this specific question, the reason that one particular, this particular organization, example organization that you saw has an agent is because it's working through a VM and production environment.

**Sisir Koppaka**:      33:47          So there are customers who focus for example, are in Coobernetti's and they would prefer using operators and interface to work through that. So we have a separate operator that works for Coobernetti's production environments, but this agent is only for production environments that have particularly only VMs.

**Harpreet Singh**:     34:03          Yeah. That was my next question actually, as a follow-up. So how do you actually handle this for past platforms?

**Sisir Koppaka**:      34:08          Oh, yeah. So we have to build all of those as actions basically. Yeah.

Speaker 1:          34:15          So how does Squadcast differ from Dynatrace?

**Sisir Koppaka**:      34:19          So we are not related to Dynatrace in the domain. We are focused on productizing a lot of the SRE infrastructure that you need. So, of course you need a Dynatrace at some point to give you inputs and we have integrations with Dynatrace, AppDynamics, but managing this workflow is the main problem that we're solving. It's very similar to Postman in that way that we are not an ID, we don't help you write code.

**Sisir Koppaka**:      34:45          Postman doesn't help you write code, right? It just helps you kind of build your APIs and then you write code somewhere else. So we help you manage the workflow of ensuring that the reliability operates in your development lifecycle, but not necessarily replacing Dynatrace or anything.

**Harpreet Singh**:     35:02          Okay. Actually I've kind of got a question, a follow-up. So if I remember your statistics, there were 26% of people who said that they could solve 50% of their problems through automation actions. Would you be able to share some insights on what those actions are or what are those types of issues?

**Sisir Koppaka**:      35:21          Yeah. So actually we have a list. This question was not like a binary question. It actually had, they had to list out what answers they wanted to solve. So a lot of these are to do with changes because of production and customers are changing, customer behavior is changing. So you have to do auto scaling or you have to do certain things to a adjust to customer's requirements.

**Sisir Koppaka**:      35:44          So it's basically categorized in those failure modes that we discussed. So there are the security issues that there are some other kinds of issues that are happening externally to the code base that requests them to change in production. And there's also bugs and stuff like that that basically they need to do rollbacks. And so majority of them are infrastructure related, application code related and external [inaudible 00:10:05].

**Harpreet Singh**:     36:06          Okay. We've got a couple more questions. So we'll take the first one.

**Sisir Koppaka**:      36:10          Sure.

**Harpreet Singh**:     36:12          As production issues are not static, how can we have predefined solutions?

**Sisir Koppaka**:      36:17          Yeah. So this is a good question. So production issues are never static and they continue to evolve. The thing is, what we have understood is that the knowledge repository of how to respond to these incidents cannot be made at points of time. It has to be actively maintained. It has to be made as you're responding to incidents. So if you are making these playbooks, you're using some answerable playbook in GitHub or in your VCS. If you plug it in here, then in this workflow, you can actually have access to when it was used, the immutable facts about how it was used to respond to an incident.

**Sisir Koppaka**:      36:54          Which is something that you don't get. So as you go along, your team will end up building a rich repository of responses. So it will not be... And we are looking at certain basic things. For example, the Grafana annotation that you saw of incident data. That could be something of a community action. It could be maintained by a community because it's not necessarily related to a particular organization, but there are a lot of actions that you might have to build on your own.

**Speaker 1**:          37:19          That could come from the other glossary as well, right?

**Sisir Koppaka**:      37:21          Exactly. Whatever is your domain-specific editor glossary that you are maintaining.

Speaker 1:          37:25          I think there's one last question. Can Squadcast work as a tracer to store distributed trace information?

**Sisir Koppaka**:      37:31          So again, we don't do it ourselves, but we integrate with something like Agar and you can pull a some kind of trace information from there. You can use, different other tools to kind of integrate with us and pull that information. We are only focused on the workflow because the problem, that's where we see the gap. Combining this human activity with machine data, which might come from tracing, APM, logging, metrics and making sure that eventually you are able to achieve your goal, which is you spend time on features, not fixing bugs.

**Harpreet Singh**:     38:08          Thank you for that. So just, we're still ahead of time. So if anybody has any question from the floor? Okay. I guess not. So thank you, Sisir.

**Sisir Koppaka**:      38:18          Thank you so much.

**Harpreet Singh**:     38:20          Here's a little token of appreciation.

**Sisir Koppaka**:      38:23          Thank you.

**Harpreet Singh**:     38:23          Thank you.

**Sisir Koppaka**:      38:24          Thank you.

**Harpreet Singh**:     38:24          All right-

